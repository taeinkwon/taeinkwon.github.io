<!DOCTYPE html>
<html lang="en">

<head>
   <meta charset="utf-8">
   <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
   <meta name="description"
      content="We propose CASA: Context-Aware Sequence Alignment using 4D Skeletal Augmentation. Based on off-the-shelf human pose estimators, we propose a novel context-aware self-supervised learning architecture to align sequences of actions.">
   <meta name="keywords"
      content="video alignment; self-supervised learning; transformer; pose alignment; 3D scene; deep learning; 3D vision; computer vision;">
   <meta name="author" content="Taein Kwon">
   <title>[CVPR 2022] Context-Aware Sequence Alignment using 4D Skeletal Augmentation</title>


   <meta name="twitter:card" content="summary_large_image" />
   <meta name="twitter:title" content="[CVPR 2022] Context-Aware Sequence Alignment using 4D Skeletal Augmentation" />
   <meta name="twitter:description"
      content="Based on off-the-shelf human pose estimators, we propose a novel context-aware self-supervised learning architecture to align sequences of actions." />
   <meta name="twitter:image" content="https://taeinkwon.github.io/projects/CASA/images/baseball_swing_all.gif" />
   <meta name="twitter:image:alt" content="CASA CVPR 2022" />

   <!-- Bootstrap core CSS -->
   <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
   <!-- Custom styles for this template -->
   <link href="css/scrolling-nav.css" rel="stylesheet">
   <!-- nice figures  -->
   <!--<link rel="stylesheet" href="css/font-awesome.css">-->
   <!--<link rel="icon" type="image/png" href="images/favicon.png">-->

   <!-- Google tag (gtag.js) -->
   <script async src="https://www.googletagmanager.com/gtag/js?id=G-FKCZFTHTJN"></script>
   <script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());

   gtag('config', 'G-FKCZFTHTJN');
   </script>
</head>

<body id="page-top">
   <!-- Navigation -->
   <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
      <div class="container">
         <a class="navbar-brand js-scroll-trigger" href="#page-top">CASA</a>
         <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
         </button>
         <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
               <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#about">About</a>
               </li>
               <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#video">Video</a>
               </li>
               <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#downloads">Downloads</a>
               </li>
               <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#citation">Citation</a>
               </li>
               <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="#team">Team</a>
               </li>
            </ul>
         </div>
      </div>
   </nav>


   <header class="bg-light text-black">
      <div class="container text-center">
         <h1>CASA </h1>
         <h2>Context-Aware Sequence Alignment using 4D Skeletal Augmentation</h2><br>
         <div id="content">
            <div id="content-inner">

               <div class="section head">

                  <div class="authors">
                     <h5>
                        <a href="https://www.taeinkwon.com">Taein
                           Kwon</a><sup>1</sup>&nbsp;
                        <a href="https://btekin.github.io/">Bugra Tekin</a><sup>2</sup>&nbsp;
                        <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu
                           Tang</a><sup>1</sup>&nbsp;
                        <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a><sup>1,2</sup>&nbsp;

                        <h5>
                  </div>

                  <div class="affiliations">
                     <h5>
                        <sup>1</sup><a href="https://inf.ethz.ch/">Department of Computer Science, ETH
                           Zurich</a>&nbsp;
                        <sup>2</sup><a
                           href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-zurich/">Microsoft MR &
                           AI Lab,
                           Zurich<br></a>
                        <h5>
                  </div>

                  <div class="venue">
                     <h5>IEEE/CVF Conference on Computer Vision and Pattern Recognition(<a
                           href="http://cvpr2022.thecvf.com/" target="_blank">CVPR</a>) 2022 (Oral)<h5>
                  </div>

                  <div class="downloads">
                     <br>
                     <h3>
                        <a class="publink" href="http://arxiv.org/abs/2204.12223" target="_blank"
                           style="text-decoration: none"> Paper <i class="fa fa-print"></i></a> &nbsp;&nbsp;
                        <a class="publink" href="https://github.com/taeinkwon/casa" target="_blank"
                           style="text-decoration: none"> Code <i class="fa fa-github"></i></a> &nbsp;&nbsp;
                        <h3>
                  </div>

               </div>


               <!-- <br> -->


            </div>
   </header>


   <section id="about" class="about-section">
      <div class="container">
         <div class="row">
            <div class="col-lg-10 mx-auto">
               <p class="lead text-justify">
               </p>
               <br> <br>
               <div class="img-wide text-center">
                  <p><img class="img-fluid" alt="teaser" src="images/teaser.png"></p>
               </div>
               <br><br>
               <h2>Abstract</h2>
               <p class="lead text-justify">
                  Temporal alignment of fine-grained human actions in videos is important for numerous applications in
                  computer vision,
                  robotics, and mixed reality. State-of-the-art methods directly learn image-based embedding space by
                  leveraging powerful
                  deep convolutional neural networks. While being straightforward, their results are far from
                  satisfactory, the aligned
                  videos exhibit severe temporal discontinuity without additional post-processing steps. The recent
                  advancements in human
                  body and hand pose estimation in the wild promise new ways of addressing the task of human action
                  alignment in videos.
                  In this work, based on off-the-shelf human pose estimators, we propose a novel context-aware
                  self-supervised learning
                  architecture to align sequences of actions. We name it CASA. Specifically, CASA employs self-attention
                  and
                  cross-attention mechanisms to incorporate the spatial and temporal context of human actions, which can
                  solve the
                  temporal discontinuity problem. Moreover, we introduce a self-supervised learning scheme that is
                  empowered by novel 4D
                  augmentation techniques for 3D skeleton representations. We systematically evaluate the key components
                  of our method.
                  Our experiments on three public datasets demonstrate CASA significantly improves phase progress and
                  Kendall's Tau scores
                  over the previous state-of-the-art methods.
               </p>


               <div class="img-wide text-center">
                  <p><img class="w-100 img-fluid" alt="baseball_swing_all" src="images/baseball_swing_8.gif">
                     Sequence alignment results (baseball swing).
                     The seven sequences are aligned with each frame (skeleton) from the reference sequence
                     by the nearest neighbor search in our trained embedding space. </p>


               </div>

               <br><br>
               <h2>Pipeline Overview</h2>
               <div class="img-wide text-center">
                  <p><img class="img-fluid" alt="pipeline overview" src="images/pipeline.png"></p>
               </div>
               <p class="lead text-justify">
                  The proposed framework takes as input a skeleton sequence S<sub>k</sub> along with its
                  spatio-temporally
                  augmented version Sâ€²<sub>k</sub>. Both sequences are encoded by temporal positional encodings.
                  Self- and cross-attentional layers learn contextual information within and across sequences with the
                  help of temporal positional encoding.
                  We employ a projection head to improve our representation quality.
                  We use a contrastive regression loss that matches a pose sequence with its 4D augmented version. For
                  the
                  downstream tasks and alignment, we use the embeddings before the projection head stage.
               </p>

               <br><br>
               <h2>Sequence Alingment Results (without post-processing)</h2>
               <p class="lead text-justify">
                  Using an attention-based architecture, our method gathers contextual information from the whole
                  sequence during alignment
                  which results in superior performance than previous approaches that rely on only local context. For
                  online applications,
                  the assumption of having the full sequence will not be valid. Therefore, we also provide results for
                  online sequence alignment.
               </p>
               <div class="row">
                  <p><img class="w-100 img-fluid" alt="tennis forehand" src="images/golf_swing.gif">
                     <img class="w-100 img-fluid" alt="pull ups" src="images/tennis_forehand_1.gif">
                  </p>
               </div>
               <br><br>

               <!--
               <h2>Online Video Alignment Results</h2>
               <p class="lead text-justify">
               Our method uses an attention-based model to capture context from all the frames to compute alignment across two
               videos. However, for online applications, the assumption of having the full sequence will not be valid.
               Therefore, we further provide results for online sequence alignment.</p>
               <div class="row">
               <p><div class="col"><img class="w-100 img-fluid" alt="tennis serve online" src="images/tennis_serve_online.gif"></div>
                  <div class="col"><img class="w-100 img-fluid" alt="baseball swing online" src="images/baseball_swing_online.gif"></div></p>
               </div>-->



               <br><br>
               <h2>Additional Sequence Alignment Results</h2>
               <p class="lead text-justify">
                  We additionally show the sequence alignment results on the H2O dataset using hand skeletons.</p>
               <div class="text-center">
                  <p><img class="w-100 img-fluid" alt="h2o" src="images/h2o_small.gif"></p>
                  <!--<p><img class="w-100 img-fluid" alt="ikea" src="images/ikea.gif"></p>-->
               </div>




            </div>
         </div>
      </div>
   </section>






   <section id="video" class="">

      <div class="container">
         <div class="row">
            <div class="col-lg-10 mx-auto">
               <h2 class="section-title-tc">Video</h2>
               <div class="embed-responsive embed-responsive-16by9">

                  <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/7-ATWx7BOo0" title=""
                     frameborder="0"
                     allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                     allowfullscreen></iframe>
               </div> <br>


               <h2 class="section-title-tc">Supplementary Video</h2>
               <div class="embed-responsive embed-responsive-16by9">

                  <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/C7cejk232JU" title=""
                     frameborder="0"
                     allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                     allowfullscreen></iframe>
               </div>
            </div>
         </div>
      </div>
   </section>


   <section id="downloads" class="">
      <div class="container">
         <div class="row">
            <div class="col-lg-10 mx-auto">
               <h2>Downloads</h2> <br>
               <h2 class="img-wide text-center">
                  <a class="publink" href="http://arxiv.org/abs/2204.12223" target="_blank"
                     style="text-decoration: none"> Paper <i class="fa fa-print"></i></a>
                  &nbsp;&nbsp;
                  <a class="publink" href="https://github.com/taeinkwon/casa" target="_blank"
                     style="text-decoration: none"> Code <i class="fa fa-github"></i></a>
                  &nbsp;&nbsp;
               </h2>
            </div>
         </div>
      </div>
   </section>



   <section id="citation" class="">
      <div class="container">
         <div class="row">
            <div class="col-lg-10 mx-auto">
               <h2 class="section-title-tc">Citation</h2>
               <br>
               <a class="publink" target="_blank" href="http://arxiv.org/abs/2204.12223"><b>
                     Context-Aware Sequence Alignment using 4D Skeletal Augmentation</b><br></a>
               Taein Kwon, Bugra Tekin, Siyu Tang, Marc Pollefeys<br>
               Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022
               <br><br>
               <pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
@InProceedings{Kwon_2022_CVPR,
author = {Kwon, Taein and Tekin, Bugra and Tang, Siyu and Pollefeys, Marc},
title = {Context-Aware Sequence Alignment Using 4D Skeletal Augmentation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2022},
pages = {8172-8182}
}</pre>
            </div>
         </div>
      </div>
   </section>




   <section id="team" class="team-section">
      <div class="container">
         <div class="row">
            <div class="col-lg-10 mx-auto">
               <h2 class="section-title-tc">Team</h2>
               <div class="text-center">
                  <table>
                     <tr>
                        <th> <img src="images/teams/taein.jpg" width="175" height="175" border="0"> </th>
                        <th> <img src="images/teams/bugra.png" width="175" height="175" border="0"> </th>
                        <th> <img src="images/teams/siyu.jpg" width="175" height="175" border="0"> </th>
                        <th> <img src="images/teams/marc.jpeg" width="175" height="175" border="0"> </th>
                     </tr>

                     <tr>
                        <td> Taein Kwon </td>
                        <td> Bugra Tekin </td>
                        <td> Siyu Tang </td>
                        <td> Marc Pollefeys </td>
                  </table>

               </div>
            </div>
         </div>
      </div>
   </section>

   <section id="contact" class="">
      <div class="container">
         <div class="row">
            <div class="col-lg-10 mx-auto">
               <h2>Contact</h2>
               <br>For questions, please contact Taein Kwon:<br><a
                  href="mailto:taein.kwon@inf.ethz.ch">taein.kwon@inf.ethz.ch</a>
            </div>
         </div>
      </div>
   </section>









   <!-- Footer -->
   <footer class="py-5 bg-dark">
      <div class="container">
         <p class="m-0 text-center text-white">Copyright &copy; ETH Zurich 2022</p>
         <p style="text-align:right;font-size:small;" class="text-white">
            template from <a href="https://markomih.github.io/">Marko Mihajlovic</a> and <a
               href="https://sanweiliti.github.io/LEMO/LEMO.html">Siwei Zhang</a>
         </p>
      </div>
      <!-- /.container -->
   </footer>
   <!-- Bootstrap core JavaScript -->
   <script src="vendor/jquery/jquery.min.js"></script>
   <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
   <!-- Plugin JavaScript -->
   <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
   <!-- Custom JavaScript for this theme -->
   <script src="js/scrolling-nav.js"></script>
</body>

</html>