<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Taein Kwon</title>

  <meta name="author" content="Taein Kwon">
  <!--<meta name="viewport" content="width=device-width, initial-scale=1">-->
  <meta name="viewport" content="“width=800”">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/favicon.png">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FKCZFTHTJN"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FKCZFTHTJN');
</script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Taein Kwon</name>
                  </p>
                  <p>I am a postdoctoral research fellow at <a href="https://www.robots.ox.ac.uk/~vgg/">VGG</a> @ Oxford working with <a href="https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ&hl=en">Prof. Andrew Zisserman</a>.
                  </p>
                  <p>
                    My research interests are Egocentric Vision, Action Recognition, Contextual AI, Hand-object Interaction, Video Understanding, AR/VR, Multi-modal Learning, Visual-language Models and Self-supervised Learning.
                  </p>
                  <p>
                    Previously, I did my PhD under the supervision of 
                    <a href="http://people.inf.ethz.ch/pomarc/">Prof. Marc Pollefeys</a> at ETH Zurich
                     and I earned my Master's degree from UCLA. I
                    received my Bachelor's in
                    Electrical Engineering from Yonsei University, Seoul, Korea
                  </p>

                  <p>
                    If you are interested in semester projects (ETHZ), master's theses (ETHZ), 4YP (Oxford), or personal projects related to action recognition,
                    egocentric vision, video understanding, and hand-object interaction that could lead to publications, feel free to email
                    me. We can discuss potential exciting projects.
                  </p>
                  <!-- <p style="color:tomato;">
                    I will finish my PhD this coming winter and I am actively looking for research positions.
                  </p>  -->

                  <p style="text-align:center">
                    <a href="mailto:taein@robots.ox.ac.uk">Email</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/taein-kwon-91022192/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://drive.google.com/file/d/10Ydqx_179EwOx9C71JLNeXf0MNnZP_7G/view?usp=sharing">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=opM0MYEAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/taeinkwon1">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/taeinkwon">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/taeinkwon.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/taeinkwon.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>

                </td>

              </tr>

            </tbody>
          </table>
          <ul>
            <li>03/2025 Our paper JEGAL is accepted to ICCV 2025. </li>
            <li>03/2025 Our paper EgoPressure is accepted to CVPR 2025 (highlight). </li>
            <li>09/2024 I will start my postdoc fellow position at VGG, Oxford. </li>
            <li>07/2024 I successfully defended my PhD. </li>
            <li>07/2024 Our paper HoloAssist received the CVPR Egovis 2022/2023 Distinguished Paper Award. </li>
            <li>05/2024 I received the prestigious SNSF Postdoc.Mobility fellowship. </li>
            <li>07/2023 Our paper HoloAssist is accepted to ICCV 2023. </li>
            <li>12/2022 I will start my research internship at Meta Reality Labs Research. </li>
            <li>06/2022 Our paper Egobody is accepted to ECCV 2022. </li>
            <li>06/2022 I will start my research internship at Microsoft Research. </li>
            <li>05/2022 I will co-organize <a class="publink" target="_blank"
                href="https://sites.google.com/view/egocentric-hand-body-activity">Human Body, Hands, and Activities
                from Egocentric and Multi-view Cameras</a> @ ECCV 2022. </li>
            <li>03/2022 Our paper CASA is accepted to CVPR 2022 (oral). </li>
            <li>03/2021 Our paper H2O is accepted to ICCV 2021. </li>
            <br>
          </ul>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>




              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='jegal'><video width=100% height=100% muted autoplay loop>
                        <!--<source src="images/egobody.mp4" type="video/mp4">-->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/EgoWorld.png' id='jegal_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://redorangeyellowy.github.io/EgoWorld/">
                    <papertitle>EgoWorld:
                      Translating Exocentric View to Egocentric View
                      using Rich Exocentric Observations
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://redorangeyellowy.github.io/">Junho Park</a>,
                  <a href="https://www.linkedin.com/in/andrew-sangwoo-ye-97a175199/">Andrew Sangwoo Ye</a>,
                  <strong>Taein Kwon</strong>
                  
                  
                  <br>

                  <em>arXiv</em>, 2025&nbsp
                  <br>
                  <!-- <a href="">project page</a> -->
                  <a href="https://redorangeyellowy.github.io/EgoWorld/">project page</a>
                  / <a href="https://arxiv.org/abs/2506.17896">paper</a>
                 
                  <p></p>
                  <p>We introduce EgoWorld, a novel two-stage framework that reconstructs egocentric view from rich exocentric observations, including depth maps, 3D hand poses, and textual descriptions.</p>
                </td>
              </tr>




              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='jegal'><video width=100% height=100% muted autoplay loop>
                        <!--<source src="images/egobody.mp4" type="video/mp4">-->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/jegal.png' id='jegal_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.robots.ox.ac.uk/~vgg/research/jegal/">
                    <papertitle>Understanding Co-speech Gestures in-the-wild
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://sindhu-hegde.github.io/">Sindhu Hggde*</a>,
                  <a href="https://www.robots.ox.ac.uk/~prajwal/">K R Prajwal*</a>,
                  <strong>Taein Kwon</strong>,
                  <a href="https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ&hl=en">Andrew Zisserman</a>
                  
                  
                  <br>

                  <em>ICCV</em>, 2025&nbsp
                  <br>
                  <!-- <a href="">project page</a> -->
                  <a href="https://www.robots.ox.ac.uk/~vgg/research/jegal/">project page</a>
                  / <a href="https://arxiv.org/abs/2503.22668">paper</a>
                  / <a href="https://github.com/Sindhu-Hegde/jegal">code</a>
                 
                  <p></p>
                  <p>We introduce JEGAL, a Joint Embedding space for Gestures, Audio and Language. Our semantic gesture representations can be used to perform multiple downstream tasks such as cross-modal retrieval, spotting gestured words, and identifying who is speaking solely using gestures.</p>
                  * co-first authors
                </td>
              </tr>



              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='egopressure'><video width=100% height=100% muted autoplay loop>
                        <!--<source src="images/egobody.mp4" type="video/mp4">-->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/egopressure.png' id='egopressure_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://yiming-zhao.github.io/EgoPressure/">
                    <papertitle>EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://yiming-zhao.github.io/">Yiming Zhao*</a>,
                  <strong>Taein Kwon*</strong>,
                  <a href="https://paulstreli.com/">Paul Streli*</a>,
                  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
                  <a href="https://www.christianholz.net/">Christian Holz</a>
                  
                  <br>

                  <em>CVPR</em>, 2025&nbsp <font color="red"><strong>(Highlight)</strong></font>
                  <br>
                  <a href="https://yiming-zhao.github.io/EgoPressure/">project page</a>
                  / <a href="https://arxiv.org/abs/2409.02224">paper</a>
                 
                  <p></p>
                  <p>We introduce EgoPressure, a novel dataset of touch contact and pressure interaction from an egocentric perspective, complemented with hand pose meshes and fine-grained pressure intensities for each contact. 
                  </p>
                  * co-first authors
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='masa'><video width=100% height=100% muted autoplay loop>
                        <!--<source src="images/egobody.mp4" type="video/mp4">-->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/masa.png' id='masa_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>Multi Activity Sequence Alignment via Implicit Clustering
                    </papertitle>
                  </a>
                  <br>
                  <strong>Taein Kwon</strong>,
                  <a href="https://paulstreli.com/">Zador Pataki</a>,
                  <a href="https://www.christianholz.net/">Mahdi Rad</a>,
                  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
                  
                  
                  <br>

                  <em>arXiv</em>, 2025&nbsp
                  <br>
                  <!-- <a href="">project page</a> -->
                  <a href="https://arxiv.org/abs/2503.12519">paper</a>
                 
                  <p></p>
                  <p>We propose a novel framework that overcomes these limitations using sequence alignment via implicit clustering. Specifically, our key idea is to perform implicit clip-level clustering while aligning frames in sequences. This coupled with our proposed dual augmentation technique enhances the network's ability to learn generalizable and discriminative representations. 
                  </p>
                </td>
              </tr>
              

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='holoassist'><video width=100% height=100% muted autoplay loop>
                        <!--<source src="images/egobody.mp4" type="video/mp4">-->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/holoassist.png' id='holoassist_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://holoassist.github.io/">
                    <papertitle>HoloAssist: An Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://xinw.ai/">Xin Wang*</a>,
                  <strong>Taein Kwon*</strong>,
                  <a href="https://radmahdi.github.io/Home.html">Mahdi Rad</a>,
                  <a href="http://people.csail.mit.edu/bpan/">Bowen Pan</a>,
                  <a href="https://scholar.google.com/citations?user=YH17Fj0AAAAJ&hl=en/">Ishani Chakraborty</a>,
                  <a href="https://seanandrist.com/">Sean Andrist</a>,
                  Ashley Fanello,
                  <a href="https://btekin.github.io/">Bugra Tekin</a>,
                  <a href="https://www.microsoft.com/en-us/research/people/fevieira/">Felipe Vieira Frujeri</a>,
                  <a href="https://neelj.com/">Neel Joshi</a>,
                    <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
                  <br>

                  <em>ICCV</em>, 2023&nbsp
                  <br>
                  <a href="https://holoassist.github.io/">project page</a>
                  / <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.pdf">paper</a>
                  / <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Wang_HoloAssist_an_Egocentric_ICCV_2023_supplemental.pdf">supp</a>

                  <p></p>
                  <p>HoloAssist is a large-scale egocentric human interaction dataset,
                    where two people collaboratively complete physical manipulation tasks. By augmenting the data with action and conversational annotations and observing the rich behaviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment.
      
                  </p>
                  * co-first authors
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='casar'><video width=100% height=100% muted autoplay loop>
                        <!--<source src="images/egobody.mp4" type="video/mp4">-->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/casar.png' id='casar_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>CaSAR: Contact-aware Skeletal Action Recognition                    </papertitle>
                  </a>
                  <br>
                  Junan Lin*,
                  Zhichao Sun*,
                  Enjie Cao*,
                  <strong>Taein Kwon</strong>,
                  <a href="https://radmahdi.github.io/Home.html">Mahdi Rad</a>,
                  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
                  <br>

                  <em>arXiv</em>, 2023&nbsp 
                  <br>
                  <!-- <a href="">project page</a>
                  / <a href="">paper</a>
                  / <a href="">code</a> -->
                  <a href="http://arxiv.org/abs/2309.10001">paper</a>
                  <p></p>
                  <p>Contact-aware Skeletal Action Recognition (CaSAR) uses novel representations
                    of hand-object interaction that encompass spatial information: 
                    1) contact points where the hand joints meet the objects, 
                    2) distant points where the hand joints are far away
                    from the object and nearly not involved in the current action. 
                    Our framework is able to learn how the hands touch or stay away from the objects 
                    for each frame of the action sequence, and use this information to predict the action class.
                  </p>
                    <!-- A student project of 3D Vision where I served as a supervisor <br> -->
                    * co-first authors
                </td>
              </tr>

              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='casar'><video width=100% height=100% muted autoplay loop>
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/imuglove.png' id='imuglove_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>Hand Microgesture Input in Virtual Reality using an IMU Glove</papertitle>
                  </a>
                  <br>
                  <strong>Taein Kwon</strong>,
                  Ergys Ristani,
                  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
                  Li Guan
                  <br>

                  <em>In review</em>, 2023&nbsp 
                  <br>
                  <p></p>
                  <p> In this project, we develop a multi-IMU glove and propose a recognition algorithm based on transformers. 
                    The algorithm utilizes both egocentric hand poses and on-glove IMU signals. 
                    We demonstrate that our proposed approach recognizes microgestures more accurately than each single modality alone. 
                    We conclude through real-world demos such as a maze game, a photo gallery, and menu interactions, 
                    that our proposed multi-IMU glove and algorithm enable more reliable and natural microgesture recognition 
                    in virtual reality.
                  </p>
                  </td>
              </tr> -->

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='egobody_video'><video width=100% height=100% muted autoplay loop>
                        <!--<source src="images/egobody.mp4" type="video/mp4">-->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/egobody.png' id='egobody_image' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sanweiliti.github.io/egobody/egobody.html">
                    <papertitle>EgoBody: Human Body Shape and Motion of Interacting People
                      from Head-Mounted Devices
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a>,
                  <a href="https://qianlim.github.io/">Qianli Ma</a>,
                  <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
                  <a href="https://ethz.ch/en.html">Zhiyin Qian</a>,
                  <strong>Taein Kwon</strong>,
                  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
                  <a href="https://fbogo.github.io/">Federica Bogo</a>,
                  <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu
                    Tang</a>
                  <br>
                  <em>ECCV</em>, 2022&nbsp
                  <br>
                  <a href="https://sanweiliti.github.io/egobody/egobody.html">project page</a>
                  / <a href="https://arxiv.org/pdf/2112.07642.pdf">paper</a>
                  / <a href="https://github.com/sanweiliti/EgoBody">code</a>

                  <p></p>
                  <!--<p>EgoBody is a large-scale dataset capturing ground-truth 3D human motions during social interactions
                    in 3D scenes. 
                    Given two interacting subjects, we leverage a lightweight multi-camera rig to reconstruct their 3D shape
                    and pose over time.</p>-->
                    <p>EgoBody is a large-scale dataset of accurate 3D human body shape, 
                      pose and motion of humans interacting in 3D scenes, with multi-modal streams from 
                      third-person and egocentric views, captured by Azure Kinects and a HoloLens2. 
                      Given two interacting subjects, we leverage a lightweight multi-camera rig to 
                      reconstruct their 3D shape and pose over time.</p>
                </td>
              </tr>



              <tr onmouseout="casa_stop()" onmouseover="casa_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='casa_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/casa.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/casa.png' id='casa_image' width="160">
                  </div>
                  <script type="text/javascript">
                    function casa_start() {
                      document.getElementById('casa_image').style.opacity = "0";
                      document.getElementById('casa_video').style.opacity = "1";
                    }

                    function casa_stop() {
                      document.getElementById('casa_image').style.opacity = "1";
                      document.getElementById('casa_video').style.opacity = "0";
                    }
                    casa_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="projects/casa/">
                    <papertitle>Context-Aware Sequence Alignment using 4D Skeletal Augmentation
                    </papertitle>
                  </a>
                  <br>
                  <strong>Taein Kwon</strong>,
                  <a href="https://btekin.github.io/">Bugra Tekin</a>,
                  <a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html">Siyu
                    Tang</a>,
                  <a href="http://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
                  <br>
                  <em>CVPR</em>, 2022&nbsp<font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="projects/casa/">project page</a>
                  / <a href="https://arxiv.org/abs/2204.12223">paper</a>
                  / <a href="https://youtu.be/7-ATWx7BOo0">video</a>
                  / <a href="https://github.com/taeinkwon/casa">code</a>

                  <p></p>
                  <p>
                    <!--Based on off-the-shelf human pose estimators, we propose a novel context-aware self-supervised learning architecture to align sequences of actions.-->
                    We propose a skeletal self-supervised learning approach that uses alignment as a pretext task. 
                    Our approach to alignment relies on a context-aware attention model that incorporates spatial 
                    and temporal context within and across sequences and a contrastive learning formulation that 
                    relies on 4D skeletal augmentations. Pose data provides a valuable cue for alignment and 
                    downstream tasks, such as phase classification and phase progression, as it is robust 
                    to different camera angles and changes in the background, while being efficient for real-time 
                    processing.
                  </p>
                </td>
              </tr>


              <tr onmouseout="h2o_stop()" onmouseover="h2o_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='h2o_video'><video width=100% height=100% muted autoplay loop>
                        <source src="images/h2o.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/h2o.png' id='h2o_image' width="160">
                  </div>
                  <script type="text/javascript">
                    function h2o_start() {
                      document.getElementById('h2o_image').style.opacity = "0";
                      document.getElementById('h2o_video').style.opacity = "1";
                    }

                    function h2o_stop() {
                      document.getElementById('h2o_image').style.opacity = "1";
                      document.getElementById('h2o_video').style.opacity = "0";
                    }
                    h2o_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="projects/h2o/">
                    <papertitle>H2O: Two Hands Manipulating Objects for First Person Interaction Recognition
                    </papertitle>
                  </a>
                  <br>
                  <strong>Taein Kwon</strong>,
                  <a href="https://btekin.github.io/">Bugra Tekin</a>,
                  <a href="https://scholar.google.com/citations?user=pGukv5YAAAAJ&hl=en">Jan Stuhmer</a>,
                  <a href="https://scholar.google.com/citations?user=1WYrlLkAAAAJ&hl=en">Federica Bogo</a>,
                  <a href="http://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>
                  <br>
                  <em>ICCV</em>, 2021
                  <br>
                  <a href="projects/h2o/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2104.11181">paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=ZGobBcsxmYQ">video</a>
                  <p></p>
                  <p><!--Our dataset, called H2O (2 Hands and Objects), provides synchronized multi-view RGB-D images,
                    interaction labels, object
                    classes, ground-truth 3D poses for left & right hands, 6D object poses, ground-truth camera poses,
                    object meshes and
                    scene point clouds. We further propose the first method to jointly recognize the 3D poses of two hands manipulating objects and a novel topology-aware graph convolutional network for recognizing hand-object interactions.</p>
                  -->
                    In this paper, we propose a method to collect a dataset of two hands manipulating objects for 
                    first person interaction recognition. We provide a rich set of annotations including action labels, 
                    object classes, 3D left & right hand poses, 6D object poses, camera poses and scene point clouds.
                     We further propose the first method to jointly recognize the 3D poses of two hands manipulating 
                     objects and a novel topology-aware graph convolutional network for recognizing hand-object interactions.
                  </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='picfridge_image'>
                      <img src='images/picfridge.png' width="160">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/2975167.2985644">
                    <papertitle>Smart Refrigerator for Healthcare Using Food Image Classification
                    </papertitle>
                  </a>
                  <br>
                  <strong>Taein Kwon</strong>,
                  <a href="https://scholar.google.co.kr/citations?user=u--k3DcAAAAJ&hl=en">Eunjeong Park</a>,
                  <a href="https://scholar.google.com/citations?user=Dkui5LsAAAAJ&hl=en">Hyukjae Chang</a>
                  <br>
                  <em>ACM BCB</em>, 2016
                  <br>
                  <p> We propose a sensor-equipped food container, Smart Refrigerator, which recognizes foods and
                    monitors their status. We demonstrated the performance in detection of food and suggested that 
                    automatic monitoring of food intake can provide intuitive feedback to users.</p>
                </td>
              </tr>

            </tbody>
          </table>

<!-- 
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Teaching</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>

                <td width="100%" valign="center">
                  Teaching Assistant
                  <ul>
                    <li><a href="http://cvg.ethz.ch/teaching/mrlab/">Mixed Reality [2019-Present]</a></li>
                    <br>
                    <li><a href="http://cvg.ethz.ch/teaching/3dvision/">3D Vision [2019-Present]</a></li>
                    <br>
                    <li><a href="http://cvg.ethz.ch/teaching/compvis/">Computer Vision [2018]</a></li>
                    <br>
                    <li>AI and new media [2018]</li>
                    <br>
                    <li>Intermediate Programming [2017]</li>
                  </ul>
                </td>
              </tr>

              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                  <tr>
                    <td>
                      <heading>Awards and Talks</heading>
                    </td>
                  </tr>
                </tbody>
              </table>
              <table width="100%" align="center" border="0" cellpadding="20">
                <tbody>

                  <tr>
                    <td width="100%" valign="center">
                      <strong>Awards</strong>
                      <ul>
                        <li>Postdoc Mobility Fellowship, Swiss National Science Foundation (155K USD) 2024</li>
                        <br>
                        <li>Distinguished Paper Award (HoloAssist), Egovis @ CVPR'24 2024</li>
                        <br>
                        <li>Grant, Swiss National Science Foundation, “Beyond Frozen Worlds: Capturing functional 3D Digital
                          Twins from the Real World” Role: Project Conceptualization PI. Prof. Marc Pollefeys (2M USD) 2023</li> 
                        <br>
                        <li>Scholarship, Recipient of Korean Government Scholarship from NIIED (150K USD) 2018</li>
                        <br>
                        <li>Scholarship, Yonsei International Foundation 2016</li>
                        <br>
                        <li>IBM Innovation Prize, Startup Weekend, Technology Competition 2015</li>
                        <br>
                        <li>Best Technology Prize, Internet of Things (IoT) Hackathon by the government of Korea 2014
                        </li>
                        <br>
                        <li>Best Laboratory Intern, Yonsei Institute of Information and Communication Technology 2014
                        </li>
                        <br>
                        <li>Scholarship, Yonsei University Foundation,Korean Telecom Group Foundation 2014, 2011, 2010</li>
                        <br>
                        <li>Creative Prize, Startup Competition, Yonsei University 2014</li>

                      </ul>
                    </td>
                  </tr>

                  <tr>
                    <td width="100%" valign="center">
                      <strong>Talks</strong>
                      <ul>

                        <li>2025/01: Video Understand Team @ Naver  
                        </li>
                        <br>
                         <li>2024/12: GSDS @ Seoul Nation University 
                        </li>
                        <br>
                         <li> 2024/07: Y.Sato Lab @ University of Tokyo 
                        </li>
                        <br>
                        <li> 2024/05: Visual Intelligence Lab @ Yonsei University 
                        </li>
                        <br>
                        <li> 2024/01: CVLAB @ Korea University 
                        </li>
                        <br>
                        <li> 2023/11: Intelligent Robotics Laboratory @ University of Birmingham 
                        </li>
                        <br>
                        <li> 2023/08: NVIDIA Research, Taiwan 
                          </li>
                          <br>
                        <li>2023/06: Microsoft Mixed Reality & AI Lab, Zurich
                        </li>
                        <br>
                        <li>2023/06:  AIoT Lab, Seoul National University
                        </li>
                        <br>
                        <li>2023/05:  KASE Open Seminar, ETH Zurich
                        </li>
                        <br>

                        <li>2022/03: Context-Aware Sequence Alignment using 4D Skeletal Augmentation. Applied Machine
                          Learning Days (AMLD) @EPFL
                          & Swiss JRC [<a
                            href="https://appliedmldays.org/events/amld-epfl-2022/talks/context-aware-sequence-alignment-using-4d-skeletal-augmentation">Link</a>]
                        </li>
                        <br>
                        <li>2021/10: H2O: Two Hands Manipulating Objects for First Person Interaction Recognition.
                          ICCV 2021 Workshop on
                          Egocentric Perception, Interaction and Computing (EPIC) [<a
                            href="https://eyewear-computing.org/EPIC_ICCV21/program">Link</a>|<a
                            href="https://www.youtube.com/watch?v=kNNuU1CKZPw&t=13632s">Video</a>]</li>
                        <br>
                        <li>2021/04: H2O: Two Hands Manipulating Objects for First Person Interaction Recognition.
                          Swiss Joint Research Center
                          (JRC) Workshop 2021 [<a
                            href="https://www.microsoft.com/en-us/research/event/joint-research-centre-workshop-2021/computer-vision/">Link</a>|<a
                            href="https://www.microsoft.com/en-us/research/video/h2o-two-hands-manipulating-objects-for-first-person-interaction-recognition-jrc-workshop-2021/">Video</a>]
                        </li>
                      </ul>
                    </td>
                  </tr>

                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                      <tr>
                        <td>
                          <heading>Academic Service</heading>
                        </td>
                      </tr>
                    </tbody>
                  </table>
                  <table width="100%" align="center" border="0" cellpadding="20">
                    <tbody>

                      <tr>
                        <td width="100%" valign="center">

                          Organizer: <a href="https://sites.google.com/view/egocentric-hand-body-activity/home">Human Body, Hands, and Activities from Egocentric and Multi-view Cameras @ ECCV'22</a>, <br> KSAE Open Seminar @ ETH Zurich
                          <br><br>

                          Reviwer: CVPR, ICCV, ECCV, CHI, SIGGRAPH
                        </td>
                      </tr>
 -->

<!-- 
                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                      <tr>
                        <td>
                          <heading>Student Projects</heading>
                        </td>
                      </tr>
                    </tbody>
                  </table>
                  <table width="100%" align="center" border="0" cellpadding="20">
                    <tbody>
    
                      <tr>
                        <td width="100%" valign="center">
                          <ul>

                            <li>Point Cloud Completion for Mixed Reality, <a href="http://cvg.ethz.ch/teaching/mrlab/">Mixed Reality 2023</a></li>
                            <br>

                            <li>Few-Shot Action Recognition, Master Semster Thesis 2023</li>
                            <br>

                            <li>Understanding Human Hand and Finger Interaction with Rigid Surfaces for
                              AR/VR Applications, Master Thesis 2023</li>
                            <br>

                            <li>Point Cloud-Based Tutorials for the HoloLens 2, Bachelor Thesis 2023</li>
                            <br>
      

                            <li>CaSAR: Contact-aware Skeletal Action Recognition, <a herf="https://cvg.ethz.ch/teaching/3dvision/">3DV 2023</a></li>
                            <br>

                            <li>Head-Worn Camera Image Stabilization using Neural Radiance Fields, <a herf="https://cvg.ethz.ch/teaching/3dvision/">3DV 2023</a></li>
                            <br>

                            <li>Object Pose Estimation and Tracking in Mixed Reality Appplications, Master Semester Thesis 2023</li>
                            <br>
                            
                            <li>Interacting with the Robot via Hololens 2, <a href="http://cvg.ethz.ch/teaching/mrlab/">Mixed Reality 2022</a></li>
                            <br>

                            <li>Climbing instructions using Mixed Reality, <a href="http://cvg.ethz.ch/teaching/mrlab/">Mixed Reality 2022</a></li>
                            <br>
                            
                            <li>Image Stabilization for HoloLens Camera, <a href="http://cvg.ethz.ch/teaching/mrlab/">Mixed Reality 2022</a></li>
                            <br>
                            
                            <li><a href="https://github.com/lukas-walker/4d-holographic-tutorials">4D Holographic Tutorials</a>, <a href="http://cvg.ethz.ch/teaching/3dvision/">3DV 2022</a></li>
                            <br> 
                            <li>AR Dance Game, <a href="http://cvg.ethz.ch/teaching/mrlab/">Mixed Reality 2021</a></li>
                            <br>
                            
                            <li>AR Dataset for Egocentric Action Recognition, <a href="http://cvg.ethz.ch/teaching/3dvision/">3DV 2021</a></li>
                            <br>
                            
                            <li>Virtual Paintings and Graffitis, <a href="http://cvg.ethz.ch/teaching/mrlab/">Mixed Reality 2020</a></li> 
                            <br>
                            
                            <li><a href="https://github.com/ShengyuH/3D-PoTion">3D Pose Motion Representation for Action Recognition</a>, <a href="http://cvg.ethz.ch/teaching/3dvision/">3DV 2019</a></li>
                            <br>

                            <li>3D Hand Shape and Pose from Images, <a href="http://cvg.ethz.ch/teaching/3dvision/">3DV 2019</a></li>
                            <br>

                            <li>HoloLens Robot Controller, <a href="http://cvg.ethz.ch/teaching/3dvision/">3DV 2019 </a></li>
                            <br>



                          </ul>
                        </td>
                      </tr> -->
    


                      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                          <tr>
                            <td>
                              <heading>Mentoring</heading>
                            </td>
                          </tr>
                        </tbody>
                      </table>
                      <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>
        
                          <tr>
                            <td width="100%" valign="center">
                              <ul>
<!-- 
                                <li>Jeongmin Bae</li>
                                <br>

                                <li>Seoha Kim</li>
                                <br> -->

                                <li>Alyssa Chan (2024-2025, master thesis/4yp, "Recognising British Sign Language in Video using Deep Learning"), Msc student at Oxford</li>
                                <br>

                                <li>Junho Park (2024-2025, collaboration project, "EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations"),  AI researcher at LG; led a submission  <a href="https://redorangeyellowy.github.io/EgoWorld/">EgoWorld</a></li>
                                <br>

                                <li>Seokjun Kim (2024-2025, collaboration project, "Teleoperating Robots in Virtual Reality"), Msc student at ETH Zurich / Neuromeka -> Now PhD student at Georgia Tech</li>
                                <br>
                                
                                <li> Minsung Kang (2024-2025, semester project, "LLM-Driven Data Augmentation and Classification for Mistake Detection in Egocentric Videos"), Msc student at ETH Zurich</li>
                                <br>
                                
                                <li> Tavis Siebert (2024-2025, 3DV project, "Gaze-Guided Scene Graphs for Egocentric Action Prediction"), Msc students at ETH Zurich</li>
                                <br>

                                <li> Dennis Baumann & Christopher Bennewitz (2024-2025, 3DV project, "Contact-Aware Action Recognition"), Msc students at ETH Zurich</li>
                                <br>

                                <li>Yiming Zhao (2023-2024, master thesis, "EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision"), Msc student at ETH Zurich; led CVPR '25 paper <a href="https://yiming-zhao.github.io/EgoPressure/">EgoPressure</a> (highlight)</li>
                                <br>

                                <li>Boub Mischa (2023-2024, semester project, "Text-Enhanced Few-Shot Learning for Egocentric Video Action Recognition"), Msc student at ETH Zurich -> Now co-founder at Swiss Engineering Partners AG </li>
                                <br>
                                
                                <li> Junan Lin & Zhichao Sun & Enjie Cao (2023, 3DV project, "CaSAR: Contact-aware Skeletal Action Recognition"), Msc students at ETH Zurich; led a technical report <a href="https://arxiv.org/abs/2309.10001">CaSAR</a> </li>
                                <br>

                                <li> Aashish Singh (2023, semester project, "Object Pose Estimation and Tracking in Mixed Reality Appplications"), Msc student at ETH Zurich </li>
                                <br>
                                
                                <li> Yanik Künzi (2023, bacher thesis, "Point Cloud-Based Tutorials for the HoloLens2"), Bsc student at ETH Zurich; led the main software development for the project’s <a href="https://gitlab.ethz.ch/ykuenzi/bachelorthesis">repository</a> </li>
                                <br>
                                

    
                              </ul>
                            </td>
                          </tr>
                  
                      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                          <tr>
                            <td>
                              <heading>Contact</heading>
                            </td>
                          </tr>
                        </tbody>
                      </table>
                      <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>

                          <tr>
                            <td width="100%" valign="center">

                              Dept. of Engineering Science
                              <br><br>
                              University of Oxford
                              <br><br>
                              Engineering and Technology Building
                              <br><br>
                              18 Banbury Rd
                              <br><br>
                              Oxford OX1 3PH
                              <br><br>
                              taein@robots (dot) ox (dot) ac (dot) uk
                            </td>
                          </tr>




                        </tbody>
                      </table>
                      <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr>
                            <td style="padding:0px">
                              <br>
                              <p style="text-align:right;font-size:small;">

                                Website template from <a href="https://jonbarron.info/">Jon Barron</a>.
                              </p>
                            </td>
                          </tr>
                        </tbody>
                      </table>
        </td>
      </tr>
  </table>
</body>

</html>